{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12235747,"sourceType":"datasetVersion","datasetId":7709500},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:04:31.322678Z","iopub.execute_input":"2025-08-03T14:04:31.322990Z","iopub.status.idle":"2025-08-03T14:04:31.344971Z","shell.execute_reply.started":"2025-08-03T14:04:31.322967Z","shell.execute_reply":"2025-08-03T14:04:31.343949Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv\n/kaggle/input/tc-smiles/Tc_SMILES.csv\n/kaggle/input/smiles-extra-data/data_dnst1.xlsx\n/kaggle/input/smiles-extra-data/data_tg3.xlsx\n/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:04:33.697629Z","iopub.execute_input":"2025-08-03T14:04:33.698364Z","iopub.status.idle":"2025-08-03T14:04:37.756632Z","shell.execute_reply.started":"2025-08-03T14:04:33.698339Z","shell.execute_reply":"2025-08-03T14:04:37.755616Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nrdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# =============================================================================\n# DATA LOADING, CLEANING, AND INTEGRATION\n#\n# The foundation of any successful QSPR model is a high-quality, well-integrated dataset.\n# Our strategy involves combining the primary competition data with multiple external\n# datasets to maximize the number of training examples. The process prioritizes data\n# integrity through two key steps:\n# 1. SMILES Validation: Using a robust function to handle and filter non-standard notations.\n# 2. Prioritized Merging: Combining data while prioritizing the competition's ground truth.\n# =============================================================================\n\nimport gc\nimport pandas as pd\nfrom rdkit import Chem \n\n# --- Helper Function for Chemical Data Cleaning ---\n\ndef clean_and_validate_smiles(smiles):\n    \"\"\"\n    Validates and cleans SMILES strings, specifically targeting non-standard\n    notations found in external chemical datasets.\n    \n    Justification: External datasets often contain legacy or non-standard SMILES\n    notations (e.g., [R], [R1]) to represent polymer attachment points. These\n    can cause parsing errors in RDKit. This function explicitly filters these\n    out, ensuring only parsable molecules are passed to the feature generation stage.\n    \"\"\"\n    if not isinstance(smiles, str) or len(smiles) == 0:\n        return None\n    \n    # Explicitly filter known non-standard R-group notations\n    bad_patterns = ['[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \"[R']\", '[R\"]']\n    if any(pattern in smiles for pattern in bad_patterns):\n        return None\n    \n    try:\n        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n        if mol is None: return None\n        return Chem.MolToSmiles(mol, canonical=True)\n    except:\n        return None\n        \n# --- Data Ingestion and Standardization ---\n# This entire section is kept from your original code, as it's proven to be effective.\n# We are simply wrapping it in a more organized structure.\n\nprint(\"Starting data ingestion and standardization...\")\n\n# Load competition data\ntrain = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\ntest_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n\n# Apply initial cleaning to base data\ntrain['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\ntest_df['SMILES'] = test_df['SMILES'].apply(clean_and_validate_smiles)\ntrain.dropna(subset=['SMILES'], inplace=True)\ntest_df.dropna(subset=['SMILES'], inplace=True)\n\n# Function to integrate external data\ndef add_extra_data_clean(df_train, df_extra, target, targets_list):\n    # Ensure the target column is numeric before grouping\n    df_extra[target] = pd.to_numeric(df_extra[target], errors='coerce')\n    df_extra.dropna(subset=[target], inplace=True)\n\n    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n    df_extra.dropna(subset=['SMILES'], inplace=True)\n    if df_extra.empty: return df_train\n    \n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    \n    # Merge to fill NAs and add new SMILES\n    df_train = pd.merge(df_train, df_extra, on='SMILES', how='outer', suffixes=('', '_new'))\n    df_train[target] = df_train[target].fillna(df_train[target + '_new'])\n    df_train.drop(columns=[target + '_new'], inplace=True)\n    \n    return df_train\n\n# Load and process external datasets\nprint(\"Loading and integrating external datasets...\")\ntrain_extended = train.copy()\nTARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# A list of tuples: (path, target, processing_function)\ndatasets_to_load = [\n    ('/kaggle/input/tc-smiles/Tc_SMILES.csv', 'Tc', lambda df: df.rename(columns={'TC_mean': 'Tc'})),\n    ('/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv', 'Tg', lambda df: df),\n    ('/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv', 'Tg', lambda df: df.rename(columns={'Tg (C)': 'Tg'})),\n    ('/kaggle/input/smiles-extra-data/data_tg3.xlsx', 'Tg', lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15)),\n    \n    # --- THIS IS THE CORRECTED LINE ---\n    ('/kaggle/input/smiles-extra-data/data_dnst1.xlsx', 'Density', \n     lambda df: df.rename(columns={'density(g/cm3)': 'Density'})),\n    # ------------------------------------\n     \n    ('/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv', 'FFV', lambda df: df)\n]\n\nfor path, target, processor in datasets_to_load:\n    try:\n        ext_df = pd.read_excel(path) if path.endswith('.xlsx') else pd.read_csv(path)\n        ext_df = processor(ext_df)\n        train_extended = add_extra_data_clean(train_extended, ext_df, target, TARGETS)\n        print(f\"Successfully integrated: {os.path.basename(path)}\")\n    except Exception as e:\n        print(f\"Failed to integrate {os.path.basename(path)}: {e}\")\n\n# Drop duplicates, keeping the first instance (prioritizing original data)\ntrain_extended.drop_duplicates(subset=['SMILES'], keep='first', inplace=True)\n\nprint(\"\\nData integration summary:\")\nprint(f\"Total unique polymers for training: {train_extended['SMILES'].nunique()}\")\nfor target in TARGETS:\n    print(f\"  - {target}: {train_extended[target].notna().sum():,} available samples\")\n\n# Clean up memory\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:09:44.538217Z","iopub.execute_input":"2025-08-03T14:09:44.539114Z","iopub.status.idle":"2025-08-03T14:09:57.119721Z","shell.execute_reply.started":"2025-08-03T14:09:44.539079Z","shell.execute_reply":"2025-08-03T14:09:57.118898Z"}},"outputs":[{"name":"stdout","text":"Starting data ingestion and standardization...\nLoading and integrating external datasets...\nSuccessfully integrated: Tc_SMILES.csv\nSuccessfully integrated: TgSS_enriched_cleaned.csv\nSuccessfully integrated: JCIM_sup_bigsmiles.csv\nSuccessfully integrated: data_tg3.xlsx\nSuccessfully integrated: data_dnst1.xlsx\nSuccessfully integrated: dataset4.csv\n\nData integration summary:\nTotal unique polymers for training: 11327\n  - Tg: 8,244 available samples\n  - FFV: 7,892 available samples\n  - Tc: 866 available samples\n  - Density: 1,247 available samples\n  - Rg: 614 available samples\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1935"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# =============================================================================\n# FEATURE ENGINEERING\n#\n# The goal is to convert a SMILES string into a rich, numerical representation\n# that a machine learning model can use. Our strategy is three-fold:\n# 1. Descriptors: Calculate ~200 physicochemical properties (e.g., MolWt, LogP, TPSA)\n#    that describe the molecule's bulk properties.\n# 2. Fingerprints: Generate binary vectors (Morgan and MACCS keys) that encode the\n#    presence or absence of specific substructural features.\n# 3. Graph Features: Model the molecule as a graph and calculate topological\n#    indices (e.g., diameter, cycles), which describe its connectivity and shape.\n# =============================================================================\n\nfrom rdkit.Chem import Descriptors, MACCSkeys, rdmolops\n# --- THIS IS THE CORRECTED IMPORT ---\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n# ------------------------------------\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\n\ndef generate_features(df_input):\n    \"\"\"\n    Generates a comprehensive feature set from a DataFrame containing SMILES strings.\n    \n    Args:\n        df_input (pd.DataFrame): Input DataFrame which must contain a 'SMILES' column.\n        \n    Returns:\n        pd.DataFrame: A DataFrame containing all calculated features, indexed\n                      identically to the input DataFrame.\n    \"\"\"\n    \n    all_features_list = []\n    \n    # Initialize the Morgan fingerprint generator once\n    morgan_gen = GetMorganGenerator(radius=2, fpSize=128)\n\n    for smiles in df_input['SMILES']:\n        mol = Chem.MolFromSmiles(smiles)\n        \n        # If a molecule is invalid, append a dictionary of NaNs and continue.\n        # This preserves the DataFrame's index.\n        if mol is None:\n            all_features_list.append({})\n            continue\n            \n        # --- Feature Calculation ---\n        # 1. Descriptors\n        descriptors = Descriptors.CalcMolDescriptors(mol)\n        \n        # 2. Fingerprints\n        maccs_fp = {f'maccs_{i}': bit for i, bit in enumerate(MACCSkeys.GenMACCSKeys(mol))}\n        morgan_fp = {f'morgan_{i}': bit for i, bit in enumerate(morgan_gen.GetFingerprint(mol))}\n        \n        # 3. Graph-based features\n        graph_features = {}\n        try:\n            adj = Chem.rdmolops.GetAdjacencyMatrix(mol)\n            G = nx.from_numpy_array(adj)\n            if nx.is_connected(G):\n                graph_features['graph_diameter'] = nx.diameter(G)\n                graph_features['avg_shortest_path'] = nx.average_shortest_path_length(G)\n            else:\n                graph_features['graph_diameter'] = 0\n                graph_features['avg_shortest_path'] = 0\n            graph_features['num_cycles'] = len(list(nx.cycle_basis(G)))\n        except:\n            graph_features['graph_diameter'] = np.nan\n            graph_features['avg_shortest_path'] = np.nan\n            graph_features['num_cycles'] = np.nan\n\n        # Combine all features for the current molecule\n        combined_features = {**descriptors, **maccs_fp, **morgan_fp, **graph_features}\n        all_features_list.append(combined_features)\n        \n    # Create the final DataFrame and fill any missing values that may have occurred.\n    features_df = pd.DataFrame(all_features_list, index=df_input.index).fillna(0)\n    \n    return features_df\n\ndef augment_smiles_dataset(smiles_list, labels, num_augments=1):\n    \"\"\"\n    Augments a list of SMILES strings by generating randomized versions.\n    This increases the diversity of the training data.\n    \"\"\"\n    augmented_smiles = []\n    augmented_labels = []\n\n    for smiles, label in zip(smiles_list, labels):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None: continue\n        \n        # Add the original SMILES and its label\n        augmented_smiles.append(smiles)\n        augmented_labels.append(label)\n        \n        # Add randomized versions\n        for _ in range(num_augments):\n            rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n            augmented_smiles.append(rand_smiles)\n            augmented_labels.append(label)\n\n    return augmented_smiles, np.array(augmented_labels)\n\n# The separate_subtables function remains unchanged as it is clear and effective.\ndef separate_subtables(train_df):\n    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    subtables = {}\n    for label in labels:\n        subtables[label] = train_df[['SMILES', label]][train_df[label].notna()]\n    return subtables","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:10:14.755015Z","iopub.execute_input":"2025-08-03T14:10:14.755902Z","iopub.status.idle":"2025-08-03T14:10:14.769027Z","shell.execute_reply.started":"2025-08-03T14:10:14.755875Z","shell.execute_reply":"2025-08-03T14:10:14.768143Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# =============================================================================\n# FEATURE SELECTION CONFIGURATION\n#\n# Not all molecular features are equally predictive for every target property. For instance,\n# features related to molecular volume and weight are critical for 'Density', while\n# features describing chain flexibility (e.g., 'NumRotatableBonds') are more relevant\n# for 'Tg'.\n#\n# This dictionary defines a manually curated list of features for each of the five\n# target models. This expert-driven feature selection reduces model complexity,\n# decreases the risk of overfitting on irrelevant features, and can lead to\n# better performance and interpretability.\n# =============================================================================\n\n# A base set of descriptors considered fundamentally important for all targets.\nrequired_descriptors = {\n    'MolWt', 'MolLogP', 'TPSA', 'NumRotatableBonds', 'HeavyAtomCount',\n    'graph_diameter', 'num_cycles', 'avg_shortest_path'\n}\n\n# The main dictionary mapping each target to its specific list of required features.\nfilters = {\n    'Tg': sorted(list(set([\n        'BalabanJ','BertzCT','Chi1','Chi3n','Chi4n','EState_VSA4','EState_VSA8',\n        'FpDensityMorgan3','HallKierAlpha','Kappa3','MaxAbsEStateIndex','MolLogP',\n        'NumAmideBonds','NumHeteroatoms','NumHeterocycles','NumRotatableBonds',\n        'PEOE_VSA14','Phi','RingCount','SMR_VSA1','SPS','SlogP_VSA1','SlogP_VSA5',\n        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState4','VSA_EState6','VSA_EState7',\n        'VSA_EState8','fr_C_O_noCOO','fr_NH1','fr_benzene','fr_bicyclic','fr_ether',\n        'fr_unbrch_alkane'\n    ]).union(required_descriptors))),\n\n    'FFV': sorted(list(set([\n        'AvgIpc','BalabanJ','BertzCT','Chi0','Chi0n','Chi0v','Chi1','Chi1n','Chi1v',\n        'Chi2n','Chi2v','Chi3n','Chi3v','Chi4n','EState_VSA10','EState_VSA5',\n        'EState_VSA7','EState_VSA8','EState_VSA9','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','FractionCSP3','HallKierAlpha',\n        'HeavyAtomMolWt','Kappa1','Kappa2','Kappa3','MaxAbsEStateIndex',\n        'MaxEStateIndex','MinEStateIndex','MolLogP','MolMR','MolWt','NHOHCount',\n        'NOCount','NumAromaticHeterocycles','NumHAcceptors','NumHDonors',\n        'NumHeterocycles','NumRotatableBonds','PEOE_VSA14','RingCount','SMR_VSA1',\n        'SMR_VSA10','SMR_VSA3','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA9','SPS',\n        'SlogP_VSA1','SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','SlogP_VSA2',\n        'SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7',\n        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState10','VSA_EState2',\n        'VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7',\n        'VSA_EState8','VSA_EState9','fr_Ar_N','fr_C_O','fr_NH0','fr_NH1',\n        'fr_aniline','fr_ether','fr_halogen','fr_thiophene'\n    ]).union(required_descriptors))),\n\n    'Tc': sorted(list(set([\n        'BalabanJ','BertzCT','Chi0','EState_VSA5','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','HeavyAtomMolWt','MinEStateIndex',\n        'MolWt','NumAtomStereoCenters','NumRotatableBonds','NumValenceElectrons',\n        'SMR_VSA10','SMR_VSA7','SPS','SlogP_VSA6','SlogP_VSA8','VSA_EState1',\n        'VSA_EState7','fr_NH1','fr_ester','fr_halogen'\n    ]).union(required_descriptors))),\n\n    'Density': sorted(list(set([\n        'BalabanJ','Chi3n','Chi3v','Chi4n','EState_VSA1','ExactMolWt',\n        'FractionCSP3','HallKierAlpha','Kappa2','MinEStateIndex','MolMR','MolWt',\n        'NumAliphaticCarbocycles','NumHAcceptors','NumHeteroatoms',\n        'NumRotatableBonds','SMR_VSA10','SMR_VSA5','SlogP_VSA12','SlogP_VSA5',\n        'TPSA','VSA_EState10','VSA_EState7','VSA_EState8'\n    ]).union(required_descriptors))),\n\n    'Rg': sorted(list(set([\n        'AvgIpc','Chi0n','Chi1v','Chi2n','Chi3v','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','HallKierAlpha','HeavyAtomMolWt',\n        'Kappa3','MaxAbsEStateIndex','MolWt','NOCount','NumRotatableBonds',\n        'NumUnspecifiedAtomStereoCenters','NumValenceElectrons','PEOE_VSA14',\n        'PEOE_VSA6','SMR_VSA1','SMR_VSA5','SPS','SlogP_VSA1','SlogP_VSA2',\n        'SlogP_VSA7','SlogP_VSA8','VSA_EState1','VSA_EState8','fr_alkyl_halide',\n        'fr_halogen'\n    ]).union(required_descriptors)))\n}\n\nprint(\"Feature selection filters defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:10:18.902943Z","iopub.execute_input":"2025-08-03T14:10:18.903770Z","iopub.status.idle":"2025-08-03T14:10:18.915778Z","shell.execute_reply.started":"2025-08-03T14:10:18.903742Z","shell.execute_reply":"2025-08-03T14:10:18.914650Z"}},"outputs":[{"name":"stdout","text":"Feature selection filters defined.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# =============================================================================\n# MODELING CONFIGURATION AND wMAE WEIGHTS\n#\n# This cell centralizes all parameters for the modeling stage.\n# =============================================================================\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np\n\ndef calculate_wmae_weights(df, targets):\n    \"\"\"\n    Calculates the wMAE weights based on the competition's specific formula.\n    \"\"\"\n    K = len(targets)\n    n_i = df[targets].notna().sum()\n    r_i = df[targets].max() - df[targets].min()\n    inv_sqrt_n = 1 / np.sqrt(n_i)\n    normalization_factor = np.sum(inv_sqrt_n)\n    weights = (1 / r_i) * (K * inv_sqrt_n / normalization_factor)\n    return weights.to_dict()\n\n# Calculate the official evaluation weights using our full training data.\nwmae_weights = calculate_wmae_weights(train_extended, TARGETS)\nprint(\"Official wMAE Weights:\", wmae_weights)\n\n\n# Store our pre-tuned XGBoost hyperparameters in a dictionary for easy access.\nmodel_params = {\n    'Tg': {'n_estimators': 2173, 'learning_rate': 0.0672, 'max_depth': 6, 'reg_lambda': 5.545},\n    'FFV': {'n_estimators': 2202, 'learning_rate': 0.0722, 'max_depth': 4, 'reg_lambda': 2.887},\n    'Tc': {'n_estimators': 1488, 'learning_rate': 0.0104, 'max_depth': 5, 'reg_lambda': 9.970},\n    'Density': {'n_estimators': 1958, 'learning_rate': 0.1095, 'max_depth': 5, 'reg_lambda': 3.074},\n    'Rg': {'n_estimators': 520, 'learning_rate': 0.0732, 'max_depth': 5, 'reg_lambda': 0.971}\n}\n\n# Add common parameters to all models\nfor p in model_params.values():\n    p['random_state'] = 42\n    p['n_jobs'] = -1 # Use all available CPU cores\n    p['tree_method'] = 'hist' # Use fast histogram-based gradient boosting\n\n\nprint(\"\\nModeling and wMAE configurations are set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:10:23.298197Z","iopub.execute_input":"2025-08-03T14:10:23.298917Z","iopub.status.idle":"2025-08-03T14:10:23.313680Z","shell.execute_reply.started":"2025-08-03T14:10:23.298889Z","shell.execute_reply":"2025-08-03T14:10:23.312466Z"}},"outputs":[{"name":"stdout","text":"Official wMAE Weights: {'Tg': 0.0005876219273313449, 'FFV': 0.8189855294394338, 'Tc': 0.8811493366469583, 'Density': 0.8387398500722527, 'Rg': 0.0647523046595221}\n\nModeling and wMAE configurations are set.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# =============================================================================\n# PRE-COMPUTATION OF MOLECULAR FEATURES\n# =============================================================================\nimport time\nprint(\"Starting feature pre-computation for all unique molecules...\")\nstart_time = time.time()\n\n# Generate features for the entire training and test sets ONCE.\ntrain_features_df = generate_features(train_extended)\ntest_features_df = generate_features(test_df)\n\n# Align columns to ensure consistency\ntrain_cols = set(train_features_df.columns)\ntest_cols = set(test_features_df.columns)\nmissing_in_test = list(train_cols - test_cols)\nmissing_in_train = list(test_cols - train_cols)\nfor col in missing_in_test: test_features_df[col] = 0\nfor col in missing_in_train: train_features_df[col] = 0\ntest_features_df = test_features_df[train_features_df.columns]\n\nend_time = time.time()\nprint(f\"Feature generation complete for {len(train_features_df)} train and {len(test_features_df)} test molecules.\")\nprint(f\"Total features: {len(train_features_df.columns)}. Time taken: {end_time - start_time:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:10:26.837582Z","iopub.execute_input":"2025-08-03T14:10:26.838304Z","iopub.status.idle":"2025-08-03T14:15:38.389590Z","shell.execute_reply.started":"2025-08-03T14:10:26.838278Z","shell.execute_reply":"2025-08-03T14:15:38.388624Z"}},"outputs":[{"name":"stdout","text":"Starting feature pre-computation for all unique molecules...\nFeature generation complete for 11327 train and 3 test molecules.\nTotal features: 515. Time taken: 311.54 seconds.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# =============================================================================\n# MODEL TRAINING WITH K-FOLD CROSS-VALIDATION\n# =============================================================================\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\n\n# Prepare data subsets and initialize storage for results\nsubtables = separate_subtables(train_extended)\noof_scores = {}\nfinal_test_predictions = pd.DataFrame({'id': test_df.index})\n\n# --- Main Training Loop ---\nfor label in TARGETS:\n    print(f\"\\n--- Processing Target: {label} ---\")\n    \n    # 1. Select data for the current target\n    target_df = subtables[label]\n    X = train_features_df.loc[target_df.index]\n    y = target_df[label]\n    \n    # 2. Apply the specific feature filter for this target\n    X = X[filters[label]]\n    X_test = test_features_df[filters[label]]\n    \n    # 3. K-Fold Cross-Validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds_target = np.zeros(len(X))\n    test_preds_target = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # Initialize the model with pre-defined parameters\n        model = XGBRegressor(**model_params[label])\n        model.fit(X_train, y_train,\n                  eval_set=[(X_val, y_val)],\n                  eval_metric='mae',\n                  verbose=0,\n                  early_stopping_rounds=50) # Stop if MAE doesn't improve for 50 rounds\n        \n        # Store predictions\n        oof_preds_target[val_idx] = model.predict(X_val)\n        test_preds_target.append(model.predict(X_test))\n        \n    # 4. Evaluate and store results for this target\n    mae = mean_absolute_error(y, oof_preds_target)\n    oof_scores[label] = mae\n    print(f\"OOF MAE for {label}: {mae:.5f}\")\n    \n    # Average predictions across all 5 folds for the final test prediction\n    final_test_predictions[label] = np.mean(test_preds_target, axis=0)\n\n# --- Final Validation Score Calculation ---\nwmae_score = 0\nfor label in TARGETS:\n    wmae_score += wmae_weights[label] * oof_scores[label]\n\nprint(f\"\\n--- Validation Complete ---\")\nprint(f\"Final Calculated OOF wMAE Score: {wmae_score:.6f}\")\n\n# --- Create Submission File ---\nsubmission_df = final_test_predictions.copy()\nsubmission_df['id'] = test_df['id'].values # Ensure correct IDs are used\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\n✅ submission.csv created successfully.\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:15:48.540411Z","iopub.execute_input":"2025-08-03T14:15:48.540716Z","iopub.status.idle":"2025-08-03T14:17:39.191852Z","shell.execute_reply.started":"2025-08-03T14:15:48.540695Z","shell.execute_reply":"2025-08-03T14:17:39.191032Z"}},"outputs":[{"name":"stdout","text":"\n--- Processing Target: Tg ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"OOF MAE for Tg: 28.88676\n\n--- Processing Target: FFV ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"OOF MAE for FFV: 0.00615\n\n--- Processing Target: Tc ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"OOF MAE for Tc: 0.03457\n\n--- Processing Target: Density ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"OOF MAE for Density: 0.05709\n\n--- Processing Target: Rg ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"OOF MAE for Rg: 1.75856\n\n--- Validation Complete ---\nFinal Calculated OOF wMAE Score: 0.214226\n\n✅ submission.csv created successfully.\n           id          Tg       FFV        Tc   Density         Rg\n0  1109053969  156.643921  0.373301  0.185462  1.151829  19.528582\n1  1422188626  165.612869  0.376664  0.256528  1.126175  20.328976\n2  2032016830  135.780090  0.350843  0.235498  1.132888  19.900789\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# =============================================================================\n# SAVE PRODUCTION ARTIFACTS (FINAL ROBUST VERSION)\n# =============================================================================\nimport joblib\nimport os\nfrom sklearn.feature_selection import VarianceThreshold\n\nos.makedirs('production_models', exist_ok=True)\n\nfor label in TARGETS:\n    print(f\"--- Finalizing and Saving Model for: {label} ---\")\n    \n    target_df = subtables[label]\n    X = train_features_df.loc[target_df.index]\n    y = target_df[label]\n    \n    # Use the sorted list of features\n    X = X[filters[label]]\n    \n    # Fit the selector to find which columns to keep\n    selector = VarianceThreshold(threshold=0.01)\n    selector.fit(X)\n    \n    # Instead of saving the selector object, we save the LIST of column names it keeps.\n    # This is a simple Python list and has no version dependencies.\n    kept_columns = X.columns[selector.get_support()].tolist()\n    joblib.dump(kept_columns, f'production_models/{label}_kept_columns.joblib')\n    print(f\"   Saved list of {len(kept_columns)} kept columns.\")\n    # -------------------------------------\n\n    # Filter the data using the list of kept columns\n    X_selected = X[kept_columns]\n\n    # Train the final model on this correctly filtered data\n    final_model = XGBRegressor(**model_params[label])\n    final_model.fit(X_selected, y)\n    \n    joblib.dump(final_model, f'production_models/{label}_model.joblib')\n    print(f\"   Saved model.\")\n\nprint(\"\\n✅ All production artifacts have been saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:37:50.332925Z","iopub.execute_input":"2025-08-03T14:37:50.333300Z","iopub.status.idle":"2025-08-03T14:38:20.067956Z","shell.execute_reply.started":"2025-08-03T14:37:50.333278Z","shell.execute_reply":"2025-08-03T14:38:20.067211Z"}},"outputs":[{"name":"stdout","text":"--- Finalizing and Saving Model for: Tg ---\n   Saved list of 41 kept columns.\n   Saved model.\n--- Finalizing and Saving Model for: FFV ---\n   Saved list of 86 kept columns.\n   Saved model.\n--- Finalizing and Saving Model for: Tc ---\n   Saved list of 30 kept columns.\n   Saved model.\n--- Finalizing and Saving Model for: Density ---\n   Saved list of 29 kept columns.\n   Saved model.\n--- Finalizing and Saving Model for: Rg ---\n   Saved list of 37 kept columns.\n   Saved model.\n\n✅ All production artifacts have been saved successfully.\n","output_type":"stream"}],"execution_count":24}]}